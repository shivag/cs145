{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "toc_visible": true,
      "authorship_tag": "ABX9TyM7JrJ/au/2SvwELw68E662",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shivag/cs145/blob/main/NanoDB_Storage_and_Querying.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Section 0] Ignore - Visualization Libraries Setup"
      ],
      "metadata": {
        "id": "skYj8S2ggJHd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def displaySectionCaption(caption, color):\n",
        "  html_string = f'<hr><strong><p style=\"color:{color};font-size:16px;\">{caption}</p.</strong>'\n",
        "  display(HTML(html_string))"
      ],
      "metadata": {
        "id": "DP64mzXgFW0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Section 1] Systems Primer\n",
        "We study a simplified IO model for HDDs and SSDs in cs145. The model will work well in practice, for our query optimzation and data layout problems. For more details on how the devices work, see Optional reads How HDDs work? How SSDs work?\n"
      ],
      "metadata": {
        "id": "X4OgEIKJk9yf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from math import ceil, log\n",
        "\n",
        "# We'll use MBs -- for basic conversion to MBs\n",
        "(MB, GB, TB, KB, Bytes) = (1.0, 1024.0, 1024.0*1024.0,\n",
        "                           1.0/1024.0, 1.0/(1024.0*1024))\n",
        "\n",
        "# 64 MB-Blocks (default)\n",
        "PageSizeMB = 64.0*MB\n",
        "size_of_types = {'int64': 8, 'int32': 4, 'double': 8, 'char': 1} # in bytes\n",
        "\n",
        "class IOdevice:\n",
        "  def __init__(self, accessTime, scanSpeed, C_w,\n",
        "               blockSizeMBs = PageSizeMB):\n",
        "    self.C_r = 1.0  # Cost of reads\n",
        "    self.C_w = C_w  # Cost of writes relative to reads\n",
        "    self.accessTime = accessTime\n",
        "    self.scanSpeed = scanSpeed\n",
        "    self.blockSizeMBs = blockSizeMBs\n",
        "\n",
        "  # Read costs: Simple IOcost model using Access time + Scan speeds\n",
        "  def read_pages_cost(self, numPages):\n",
        "    # Assume you need to read full pages. (i.e., no partial pages)\n",
        "    numPages = math.ceil(numPages)\n",
        "    tsecs = numPages*self.accessTime  # time to access\n",
        "    tsecs += numPages*self.blockSizeMBs/self.scanSpeed # time to scan\n",
        "    return (tsecs)\n",
        "\n",
        "  def write_pages_cost(self, numPages):\n",
        "    return self.C_w*self.read_pages_cost(numPages)\n",
        "\n",
        "# Example IO devices in 2023\n",
        "# Access and Scan speeds in [seconds, MBps], Cw cost of write vs reads.\n",
        "ram1 = IOdevice(100*pow(10, -9), 100.0*1024, 1.0)\n",
        "ssd1 = IOdevice(10*pow(10, -6), 5.0*1024, 1.0) # 10 microsecs, 5GBps\n",
        "hdd1 = IOdevice(10*pow(10, -3), 100.0, 1.0) # 10 millissecs, 100 MBps\n",
        "\n",
        "IOdevices1 = {'HDD': hdd1, 'SSD': ssd1, 'RAM': ram1}"
      ],
      "metadata": {
        "id": "AOTETQS8lLzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Basic physical table\n",
        "\"\"\"\n",
        "class Table:\n",
        "  def __init__(self, sizeInMBs, rowSize):\n",
        "    self.sizeInMBs = sizeInMBs\n",
        "    self.rowSize = rowSize\n",
        "    self.numRows = ceil(self.sizeInMBs/self.rowSize)\n",
        "\n",
        "    # self.numTuples = numTuples\n",
        "    self.isSorted = False\n",
        "    self.isHPed = False\n",
        "\n",
        "  # P(R) -- number of Pages for R\n",
        "  def P(self):\n",
        "    P = ceil(self.sizeInMBs/PageSizeMB)\n",
        "    return P\n",
        "  def RowSize(self):\n",
        "    return self.rowSize\n",
        "  def T(self):\n",
        "    return self.numRows\n",
        "  def SizeInMBs(self):\n",
        "    return self.sizeInMBs\n",
        "\n",
        "  # Keeping track of is table sorted, HPed, or neither (default)\n",
        "  def Sort(self):\n",
        "    self.isSorted = True\n",
        "    self.isHPed = False\n",
        "  def HP(self):\n",
        "    self.isSorted = False\n",
        "    self.isHPed = True\n",
        "  def Reset(self):\n",
        "    self.isSorted = False\n",
        "    self.isHPed = False\n"
      ],
      "metadata": {
        "id": "nvq0f4B0aiNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AkxlCfX-nTuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [1.1] Examples"
      ],
      "metadata": {
        "id": "IA3o5Jfltj0X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Spotify Songs Table [songid: int64, title: text, name: text, genre: text]\n",
        "#    -- Size of row = 8 bytes (int64) + avg size of title+name+genre.\n",
        "#    -- Assume avg row size = 1024 Bytes\n",
        "songs_rowSize = 1024.0*Bytes\n",
        "songs_numRows = 500000000.0 # 500 million songs\n",
        "\n",
        "\"\"\"Problem 1:\n",
        "Calculate the size (MBs) of SongsTable, and num pages.\"\"\"\n",
        "songs_sizeinMBs = songs_rowSize*songs_numRows/MB\n",
        "SongsTable = Table(songs_sizeinMBs, 1024.0)\n",
        "print(f'Problem1: {SongsTable.SizeInMBs()=} (MBs), {SongsTable.P()=} pages')\n",
        "\n",
        "\"\"\"Problem 2: Read costs\n",
        "Compute the cost in seconds to read 100 pages from the SongsTable\"\"\"\n",
        "print(f'Problem2:')\n",
        "print(f'  {ram1.read_pages_cost(100)=} secs')\n",
        "print(f'  {ssd1.read_pages_cost(100)=} secs')\n",
        "print(f'  {hdd1.read_pages_cost(100)=} secs')\n",
        "\n",
        "\"\"\"Problem 3: Effect of caching\n",
        "Read 20 pages. 1st check RAM. 90% are in RAM. 10% need to be paged from SSD.\"\"\"\n",
        "cost = ram1.read_pages_cost(20)\n",
        "cost += ssd1.read_pages_cost(0.1*20)\n",
        "print(f'Problem3: 90% cache hit rate for 20 pages (secs): {cost}' )\n",
        "\n",
        "\"\"\"Problem 4:\n",
        "Suppose you need to read 30 pages and write 10 blocks to the SongsTable.\n",
        "Calculate the total cost (secs) if the table is on different IO devices.\"\"\"\n",
        "num_read_pages = 30\n",
        "num_write_pages = 10\n",
        "print(f'Problem4:')\n",
        "for io in [ram1, ssd1, hdd1]:\n",
        "  read_cost = io.read_pages_cost(num_read_pages)\n",
        "  write_cost = io.write_pages_cost(num_write_pages)\n",
        "  total_cost = read_cost + write_cost\n",
        "  print(f\"  Cost for read {num_read_pages} + write {num_write_pages} pages: \\\n",
        "          {total_cost} secs\")"
      ],
      "metadata": {
        "id": "q4xYvPJKOrD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Section 2] NanoDB: How to store DbFiles and execute JOINs"
      ],
      "metadata": {
        "id": "8rjI5VsV923f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, HTML\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.axes as axes\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "from enum import Enum\n",
        "\n",
        "class Verbose(Enum):\n",
        "  SILENT = 0\n",
        "  NORMAL = 1\n",
        "  VERBOSE = 2\n",
        "  LISTPAGES = 3\n",
        "  DRAWPAGES = 4\n",
        "\n",
        "# adjust verbose mode for more debug information\n",
        "verbosity = Verbose.LISTPAGES\n",
        "\n",
        "##\n",
        "# DbFile: Mimic how to store data on disk, managed by a RAM buffer.\n",
        "class DbFile:\n",
        "    def __init__(self, values=None, pages=None, k=None, filepfx = ''):\n",
        "      self.filepfx = filepfx\n",
        "      self._pad = (None, None)\n",
        "      if pages: # Construct new pages based on copying pages\n",
        "        self.pages = pages\n",
        "        self.k = len(pages[0])\n",
        "        self.P = len(pages)\n",
        "        self.n = 0\n",
        "        for i, p in enumerate(pages):\n",
        "          self.n += len(p)\n",
        "          while len(p) < self.k:\n",
        "            self.pages[i].append(self._pad)\n",
        "      elif not values: # Empty page\n",
        "        self.pages = []\n",
        "        self.P = 1\n",
        "        self.k = k\n",
        "        self.n = 0\n",
        "      else: # New page based on copying values\n",
        "        self.k = k\n",
        "        self.n = len(values)\n",
        "        self.pages = [[self._pad] * k for i in range(math.ceil(len(values)/self.k))]\n",
        "        self.P = math.ceil(len(values)/k)\n",
        "        for i, value in enumerate(values):\n",
        "          page_num, index = divmod(i, k)\n",
        "          self.pages[page_num][index] = value\n",
        "\n",
        "    # Mimics reading num-th page.\n",
        "    #    If the page is in RAM, return the same page. If not, read from disk.\n",
        "    def read_page(self, page_num):\n",
        "      if self.pages == []:\n",
        "        return []\n",
        "      return self.pages[page_num]\n",
        "    # Mimics writing num-th page with page_data.\n",
        "    #    If page in RAM, change the values in page.\n",
        "    #    If not, read page from disk, and update values.\n",
        "    def write_page(self, page_num, page_data):\n",
        "        self.pages[page_num] = page_data\n",
        "\n",
        "    # Update a specific page value\n",
        "    def update_page(self, idx, new_val):\n",
        "      pnum, pidx = divmod(idx, self.k)\n",
        "      page_data = self.pages[pnum]\n",
        "      page_data[idx] = new_val\n",
        "      self.write_page(pnum, page_Data)\n",
        "\n",
        "    # Read i-th row. I.e., find (page, location in page) and return row\n",
        "    def read_ith(self, idx):\n",
        "      pnum, pidx = divmod(idx, self.k)\n",
        "      return self.read_page(pnum)[pidx]\n",
        "\n",
        "    def is_row_valid(self, row):\n",
        "      return row != self._pad\n",
        "\n",
        "    # Append values to the end of the DbFile\n",
        "    def append_page(self, value):\n",
        "      # add to last page. Or create new page\n",
        "      def new_page(value):\n",
        "        # need new page\n",
        "        newpage = [(None, None)] * self.k\n",
        "        newpage[0] = value\n",
        "        self.pages.append([value] + [(None, None)]*(self.k-1))\n",
        "        self.P = len(self.pages)\n",
        "      self.n += 1\n",
        "      if len(self.pages) > 0:\n",
        "        last_page = self.pages[self.P -1]\n",
        "        for i in range(len(last_page)):\n",
        "          if last_page[i] == (None, None):\n",
        "            last_page[i] = value\n",
        "            return\n",
        "      new_page(value)\n",
        "\n",
        "    # Splits files into smaller files\n",
        "    def split_files(self, num_pages_per_split):\n",
        "      split_files = []\n",
        "      for i in range(0, self.P, num_pages_per_split):\n",
        "        end = min(i+num_pages_per_split, self.P)\n",
        "        tmpp = self.pages[i:end]\n",
        "        split_files.append(\n",
        "            DbFile(pages=tmpp,\n",
        "                   k=self.k, filepfx = self.filepfx+str(i)))\n",
        "      return split_files\n",
        "\n",
        "    ## Functions for visuals\n",
        "    # Below is visualization code [optional read]\n",
        "    def print_pages(self):\n",
        "      for p in range(self.P):\n",
        "        print(\"Page\", p, \":\", self.read_page(p))\n",
        "    def read_all_pages(self):\n",
        "      return [self.read_page(p) for p in range(self.P)]\n",
        "      ## Helper functions for us to see pages\n",
        "    def format_cell(self, series):\n",
        "      return ', '.join(str(val) for val in series)\n",
        "\n",
        "    def print_file(self):\n",
        "      if verbosity == Verbose.DRAWPAGES:\n",
        "        self.draw_file()\n",
        "        return\n",
        "      tdf = self.read_all_pages()\n",
        "      dfList = [[self.filepfx, len(tdf), self.format_cell(tdf)]]\n",
        "      df = pd.DataFrame(dfList, columns=[\"DbFiles\", 'Num Pages', \"Pages\"])\n",
        "      display(HTML(\n",
        "          df.to_html(index=False, classes=['table', 'table-bordered'],\n",
        "                     header=['<th style=\"word-wrap: break-word; max-width: 30px;\">Pages</th>'])))\n",
        "\n",
        "    def draw_file(self):\n",
        "      pages = self.read_all_pages()\n",
        "      max_pages = max(len(pages), 50)  # or some other number that's guaranteed to be larger than your number of pages\n",
        "      max_values_per_page = max(len(page) for page in pages)\n",
        "\n",
        "      fig, axs = plt.subplots(1, max_pages, figsize=(20, 4))\n",
        "\n",
        "      # Create a color map to shade squares based on value\n",
        "      cmap = plt.get_cmap(\"Blues\")\n",
        "\n",
        "      # A normalization instance to scale data values to the [0.0, 1.0] range.\n",
        "      norm = mcolors.Normalize(vmin=0, vmax=1000)\n",
        "\n",
        "      for i in range(max_pages):\n",
        "        axs[i].set_xlim(0, 1)  # Set x-axis limits to ensure squares are visible\n",
        "        axs[i].set_ylim(0, max_values_per_page)  # Set y-axis limits based on number of rows in a page\n",
        "        axs[i].set_aspect('equal', adjustable='box')  # Set aspect ratio to ensure squares are square\n",
        "        axs[i].axis('off')  # Remove axis\n",
        "\n",
        "        if i < len(pages):\n",
        "          page = pages[i]\n",
        "          axs[i].set_title(f'{i+1}')  # Title each subplot with the page number\n",
        "\n",
        "          # Iterate over each row in a page\n",
        "          for j, row in enumerate(page):\n",
        "            if row[0] is not None:\n",
        "              square = plt.Rectangle((0.1, max_values_per_page-j-1+0.1),\n",
        "                                     0.8, 0.8, color=cmap(norm(row[0])))\n",
        "              axs[i].add_patch(square)  # Add the square to the current axes\n",
        "              axs[i].text(0.1, max_values_per_page-j-1+0.2,\n",
        "                          str(row[0]), ha='left', va='center', color='grey')\n",
        "\n",
        "          axs[i].add_patch(plt.Rectangle((0.05,0.05), 0.9,\n",
        "                                         max_values_per_page-0.1, fill=None,\n",
        "                                         edgecolor='grey'))\n",
        "      #plt.gcf().subplots_adjust(bottom=0.2)\n",
        "      #plt.text(0, -0.05, f'DbFile {self.filepfx}', fontsize=16, ha='left',\n",
        "      #         transform=plt.gcf().transFigure)\n",
        "      plt.tight_layout(pad=0.1)  #  padding between subplots\n",
        "      plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "x6MME0N4YYue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "  Algorithms supported in a DB for JOINs, sorting, hashing, etc\"\"\"\n",
        "class Algos:\n",
        "  def __init__(self, B, verbose=False):\n",
        "    self.B = B\n",
        "    self.verbose = verbose\n",
        "    return\n",
        "\n",
        "  ##\n",
        "  # Functions for Sorting\n",
        "  def split(self, R):\n",
        "    return R.split_files(self.B)\n",
        "\n",
        "  # Sort a given (small) file in RAM\n",
        "  def sortRAM(self, v_list, R):\n",
        "    concat = []\n",
        "    for l in v_list: # only use valid rows (ignore padded 'None' rows)\n",
        "      l = list(filter(lambda x: R.is_row_valid(x), l))\n",
        "      concat += l\n",
        "    s_concat = sorted(concat, key=lambda x:x[0])\n",
        "    sublists = [s_concat[i:i + R.k] for i in range(0, len(s_concat), R.k)]\n",
        "    return DbFile(pages=sublists, k=R.k, filepfx=R.filepfx)\n",
        "\n",
        "  # MergeBWay(Rlist): Merge B partially sorted files, into a bigger sorted file\n",
        "    # Step 1: Read 1st page of each of the B files. Create a small heap in RAM.\n",
        "    # Step 2: Repeat below steps until all data is sorted\n",
        "    # Step 2a:   Select smallest value from the heap, Append to the output file\n",
        "    # Step 2b:   Read the next value from the file associated with value in 2a\n",
        "    #            and add it into heap\n",
        "  def mergeBway(self, Rlist):\n",
        "    if not Rlist:\n",
        "      return []\n",
        "    out = DbFile(values=None, k=Rlist[0].k, filepfx=Rlist[0].filepfx)\n",
        "    heaps = []\n",
        "    # Step 1: Read 1st values.\n",
        "    # Also, keep a heap to track values, and which file/page they are from\n",
        "    for b in range(len(Rlist)):\n",
        "      page = Rlist[b].read_page(0)\n",
        "      # value from (sort key, row, bth file, 0th page, 0th slot)\n",
        "      heapq.heappush(heaps, (page[0][0], page[0], b, 0))\n",
        "\n",
        "    while (heaps): # Step 2a\n",
        "      cur_min, row, fnum, vidx = heapq.heappop(heaps)\n",
        "      out.append_page(row)\n",
        "      # Step 2b: Read the next value (after cur_min in same file)\n",
        "      vidx += 1\n",
        "      if vidx < Rlist[fnum].n:\n",
        "        row = Rlist[fnum].read_ith(vidx)\n",
        "        heapq.heappush(heaps, (row[0], row, fnum, vidx))\n",
        "    return [out]\n",
        "\n",
        "  # BigSort(R): Sort a big file (does not fit in RAM)\n",
        "    # 1. Split big file into many small files of B pages each\n",
        "    # 2. Sort each small file (in RAM)\n",
        "    # 3. Merge B sorted files at a time into bigger files. Repeat until done\n",
        "  def BigSort(self, R):\n",
        "    # Step 1 and 2\n",
        "    split_files = self.split(R)\n",
        "    merged_files = [[self.sortRAM(fsplit.read_all_pages(), R) \\\n",
        "                     for fsplit in split_files]]\n",
        "    numpass = 0\n",
        "\n",
        "    # Step 3\n",
        "    while (len(merged_files[numpass]) > 1):\n",
        "      mfiles = []\n",
        "      for start in range(0, len(merged_files[numpass]), self.B):\n",
        "        end = min(start + self.B, len(merged_files[numpass]))\n",
        "        mfiles += self.mergeBway(merged_files[numpass][start:end])\n",
        "      merged_files.append(mfiles)\n",
        "      numpass += 1\n",
        "    self.print_files(merged_files, add_sfx=True,\n",
        "                     caption=f'Big Sort {R.filepfx}',\n",
        "                     subcaption=f'[Pass #]')\n",
        "    return merged_files[numpass]\n",
        "\n",
        "  # SMJ(R): SortMerge Join R and S\n",
        "    # Step 1: Sort R and S (stored as DbFiles)\n",
        "    # Step 2: Iterate thro' each page of R and S, in sequence\n",
        "  def SMJ(self, inR, inS, outk):\n",
        "    # Step1\n",
        "    R = self.BigSort(inR)[0]; S = self.BigSort(inS)[0]\n",
        "\n",
        "    out_file = DbFile(values=[], k = outk, filepfx=R.filepfx + \"-\" + S.filepfx)\n",
        "    ridx = 0; sidx = 0\n",
        "\n",
        "    # Step 2: Merge 2 sorted files.\n",
        "    while (ridx < R.n and sidx < S.n):\n",
        "      r = R.read_ith(ridx); s = S.read_ith(sidx)\n",
        "      if r[0] < s[0]:\n",
        "        ridx += 1\n",
        "      if r[0] > s[0]:\n",
        "        sidx += 1\n",
        "      if r[0] == s[0]:\n",
        "        # Scan through all equal values. Backup when necessary\n",
        "        # E.g., R=[... 'bb', 'bb', 'cc', ...] and S=[...'bb', 'cc', 'cc', ...]\n",
        "        # Make sure to match all 'bb' and 'cc's.\n",
        "        backup_sidx = sidx\n",
        "        while r[0] == s[0] and sidx < S.n:\n",
        "          out_file.append_page((r[0],)+r[1:]+s[1:])\n",
        "          sidx += 1\n",
        "          s = S.read_ith(sidx)\n",
        "        ridx += 1\n",
        "        sidx = backup_sidx\n",
        "\n",
        "    self.print_files([[out_file]],\n",
        "                     caption=f'SMJ: {inR.filepfx} JOIN {inS.filepfx}')\n",
        "    return out_file\n",
        "\n",
        "  ## Functions for Hashing\n",
        "  #\n",
        "  # HP(R): Hash partition a big file\n",
        "    # Step 1: Set up B partitions (stored as DbFiles)\n",
        "    # Step 2: Read each page of R into RAM.\n",
        "    # Step 3: Hash each value h(v). Append to the h(v)th partition.\n",
        "  def HashP(self, R):\n",
        "    # internal fn to hash a given integer or string\n",
        "    import hashlib\n",
        "    def hash_value(value):\n",
        "      if isinstance(value, int):\n",
        "        return hash(value) % self.B\n",
        "      sha256 = hashlib.sha256()\n",
        "      sha256.update(value.encode('utf-8'))\n",
        "      return int(sha256.hexdigest(), 16) % self.B\n",
        "\n",
        "    ## Step 1\n",
        "    hash_files = [DbFile(values=[], k=R.k, filepfx=R.filepfx + f'.{i}') \\\n",
        "                  for i in range(self.B)]\n",
        "    # Steps 2 and 3\n",
        "    for i in range(R.P):\n",
        "      page = R.read_page(i) ## Step 2\n",
        "      for j in range(len(page)): ## process values in RAM\n",
        "        if R.is_row_valid(page[j]):\n",
        "          h = hash_value(page[j][0])\n",
        "          hash_files[h].append_page(page[j]) # Step 3\n",
        "    self.print_files([hash_files],\n",
        "                     caption=f'HashPartiton: {R.filepfx}')\n",
        "    return hash_files\n",
        "\n",
        "\n",
        "  # HPJ(R, S): Join R and S with Hash Partition Joins\n",
        "    # Step 1: HP(R), HP(S)\n",
        "    # Step 2: For each partition, run BNLJ(). Finally append\n",
        "  def HPJ(self, R, S, outk):\n",
        "    # Step 1\n",
        "    rhash = self.HashP(R)\n",
        "    shash = self.HashP(S)\n",
        "    hpjout = []\n",
        "    # Step 2\n",
        "    for i in range(self.B):\n",
        "      hpjout.append(self.BNLJ(rhash[i], shash[i], outk))\n",
        "\n",
        "\n",
        "  # BNLJ(R, S): Joins R and S using a block-nested loop algo\n",
        "    # Step 1. Read R's pages into RAM, 'B' pages at a time.\n",
        "    #    (And cache these 'B' pages in RAM)\n",
        "    # Step 2. Read S's pages to join with R's cached pages.\n",
        "  def BNLJ(self, R, S, outk):\n",
        "    out = DbFile(values=[], k=outk,\n",
        "                 filepfx=R.filepfx + '-join-' + S.filepfx)\n",
        "    for i in range(0, R.P, self.B): # Step 1\n",
        "      pages_R = [R.read_page(j) \\\n",
        "                 for j in range(i, min(i + self.B, R.P))] # IO-cost\n",
        "      for j in range(S.P): # Step 2\n",
        "        page_S = S.read_page(j) # IO-cost\n",
        "        for p in pages_R: # Below is in RAM. I.e. IO-cost = 0\n",
        "          for r in p:\n",
        "            if not R.is_row_valid(r):\n",
        "              continue\n",
        "            for s in page_S:\n",
        "              if not S.is_row_valid(s) or r[0] != s[0]:\n",
        "                continue\n",
        "              out.append_page((r[0],) + r[1:] + s[1:]) # IO-cost to append\n",
        "    self.print_files([[out]],\n",
        "                     caption=f'BNLJ: {R.filepfx} JOIN {S.filepfx}')\n",
        "    return out\n",
        "\n",
        "  def format_cell(self, series):\n",
        "    return ', '.join(str(val) for val in series)\n",
        "\n",
        "  def print_files(self, mfiles, add_sfx=False, caption=\"\", subcaption=\"\"):\n",
        "    displaySectionCaption(caption, 'blue')\n",
        "    for i, mfile in enumerate(mfiles):\n",
        "      pd.options.display.max_colwidth = 10\n",
        "      displaySectionCaption(f'====>> {subcaption} {i}', 'blue')\n",
        "      for j, dfile in enumerate(mfile):\n",
        "        filesfx = '-' + str(i) + \"-\" + str(j) if (add_sfx) else \"\"\n",
        "        dfile.filepfx += filesfx\n",
        "        dfile.print_file()\n"
      ],
      "metadata": {
        "id": "GIGquxTSjsFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "pages = [(188, 'kg', '..'), (492, 'oa', '..'), (359, 'cl', '..')], [(163, 'kn', '..'), (458, 'dx', '..'), (210, 'wl', '..')], [(123, 'cm', '..'), (494, 'bd', '..'), (486, 'vb', '..')], [(474, 'nt', '..'), (358, 'vu', '..'), (35, 'ez', '..')], [(21, 'lk', '..'), (288, 'nk', '..'), (229, 'sq', '..')], [(450, 'gm', '..'), (102, 'ch', '..'), (214, 'nx', '..')], [(126, 'my', '..'), (203, 'kz', '..'), (169, 'ak', '..')], [(234, 'yc', '..'), (357, 'in', '..'), (396, 'at', '..')], [(167, 'uw', '..'), (232, 'pm', '..'), (230, 'oa', '..')], [(131, 'fz', '..'), (477, 'gi', '..'), (327, 'zs', '..')], [(29, 'uh', '..'), (305, 'hp', '..'), (124, 'xi', '..')], [(168, 'sq', '..'), (489, 'uc', '..'), (279, 'fz', '..')], [(363, 'ma', '..'), (80, 'br', '..'), (318, 'oa', '..')], [(320, 'ut', '..'), (101, 'ya', '..'), (197, 'rm', '..')], [(402, 'mn', '..'), (255, 'uf', '..'), (246, 'ir', '..')], [(14, 'nc', '..'), (484, 'xd', '..'), (301, 'cp', '..')], [(243, 'hx', '..'), (18, 'lx', '..'), (328, 'jf', '..')], [(446, 'ew', '..'), (178, 'qf', '..'), (192, 'pc', '..')], [(432, 'it', '..'), (179, 'dg', '..'), (77, 'fq', '..')], [(41, 'na', '..'), (379, 'oj', '..'), (232, 'pk', '..')], [(194, 'ro', '..'), (110, 'um', '..'), (494, 'jf', '..')], [(375, 'ur', '..'), (177, 'fg', '..'), (453, 'lq', '..')], [(364, 'mq', '..'), (80, 'zy', '..'), (359, 'mj', '..')], [(292, 'xe', '..'), (207, 'hh', '..'), (134, 'mp', '..')], [(440, 'fi', '..'), (419, 'qp', '..'), (428, 've', '..')], [(383, 'rx', '..'), (166, 'aa', '..'), (266, 'bd', '..')], [(242, 'oc', '..'), (137, 'rd', '..'), (25, 'me', '..')], [(194, 'iq', '..'), (66, 'at', '..'), (261, 'dr', '..')], [(310, 'un', '..'), (491, 'uu', '..'), (416, 'nl', '..')], [(299, 'ha', '..'), (353, 'fr', '..'), (85, 'gk', '..')], [(141, 'yb', '..'), (96, 'yo', '..'), (23, 'ba', '..')], [(167, 'zp', '..'), (70, 'vt', '..'), (499, 'dh', '..')], [(63, 'iv', '..'), (429, 'al', '..'), (170, 'hy', '..')], [(17, 'oa', '..'), (None, None), (None, None)]\n",
        "\n",
        "pages2 = [(14, 'nc', '..'), (17, 'oa', '..'), (18, 'lx', '..')], [(21, 'lk', '..'), (23, 'ba', '..'), (25, 'me', '..')], [(29, 'uh', '..'), (35, 'ez', '..'), (41, 'na', '..')], [(63, 'iv', '..'), (66, 'at', '..'), (70, 'vt', '..')], [(77, 'fq', '..'), (80, 'br', '..'), (80, 'zy', '..')], [(85, 'gk', '..'), (96, 'yo', '..'), (101, 'ya', '..')], [(102, 'ch', '..'), (110, 'um', '..'), (123, 'cm', '..')], [(124, 'xi', '..'), (126, 'my', '..'), (131, 'fz', '..')], [(134, 'mp', '..'), (137, 'rd', '..'), (141, 'yb', '..')], [(163, 'kn', '..'), (166, 'aa', '..'), (167, 'uw', '..')], [(167, 'zp', '..'), (168, 'sq', '..'), (169, 'ak', '..')], [(170, 'hy', '..'), (177, 'fg', '..'), (178, 'qf', '..')], [(179, 'dg', '..'), (188, 'kg', '..'), (192, 'pc', '..')], [(194, 'iq', '..'), (194, 'ro', '..'), (197, 'rm', '..')], [(203, 'kz', '..'), (207, 'hh', '..'), (210, 'wl', '..')], [(214, 'nx', '..'), (229, 'sq', '..'), (230, 'oa', '..')], [(232, 'pk', '..'), (232, 'pm', '..'), (234, 'yc', '..')], [(242, 'oc', '..'), (243, 'hx', '..'), (246, 'ir', '..')], [(255, 'uf', '..'), (261, 'dr', '..'), (266, 'bd', '..')], [(279, 'fz', '..'), (288, 'nk', '..'), (292, 'xe', '..')], [(299, 'ha', '..'), (301, 'cp', '..'), (305, 'hp', '..')], [(310, 'un', '..'), (318, 'oa', '..'), (320, 'ut', '..')], [(327, 'zs', '..'), (328, 'jf', '..'), (353, 'fr', '..')], [(357, 'in', '..'), (358, 'vu', '..'), (359, 'cl', '..')], [(359, 'mj', '..'), (363, 'ma', '..'), (364, 'mq', '..')], [(375, 'ur', '..'), (379, 'oj', '..'), (383, 'rx', '..')], [(396, 'at', '..'), (402, 'mn', '..'), (416, 'nl', '..')], [(419, 'qp', '..'), (428, 've', '..'), (429, 'al', '..')], [(432, 'it', '..'), (440, 'fi', '..'), (446, 'ew', '..')], [(450, 'gm', '..'), (453, 'lq', '..'), (458, 'dx', '..')], [(474, 'nt', '..'), (477, 'gi', '..'), (484, 'xd', '..')], [(486, 'vb', '..'), (489, 'uc', '..'), (491, 'uu', '..')], [(492, 'oa', '..'), (494, 'bd', '..'), (494, 'jf', '..')], [(499, 'dh', '..'), (None, None), (None, None)]\n",
        "\n",
        "pages3 = [(3, 'kf', '..'), (13, 'pf', '..'), (14, 'eq', '..'), (15, 'tm', '..'), (41, 'zx', '..'), (63, 'fw', '..'), (73, 'fk', '..')], [(80, 'sy', '..'), (86, 'tf', '..'), (95, 'qp', '..'), (122, 'ws', '..'), (143, 'cd', '..'), (158, 'zf', '..'), (177, 'tr', '..')], [(183, 'tm', '..'), (195, 'ki', '..'), (207, 'kx', '..'), (249, 'io', '..'), (265, 'kv', '..'), (341, 'op', '..'), (343, 'uh', '..')], [(345, 'pd', '..'), (373, 'mn', '..'), (433, 'pi', '..'), (442, 'ne', '..'), (466, 'ob', '..'), (467, 'jp', '..'), (468, 'ik', '..')]\n",
        "#color_print_file(pages)\n",
        "#color_print_file(pages2)\n",
        "#color_print_file(pages3)\n"
      ],
      "metadata": {
        "id": "BkG9zfHhpj9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [2.1] Examples for Sorting, Hashing, JOINs"
      ],
      "metadata": {
        "id": "QV-2r-Cqz3H4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random, heapq, string\n",
        "\n",
        "def gen_random_string(string_length):\n",
        "    letters = string.ascii_lowercase\n",
        "    return ''.join(random.choice(letters) for i in range(string_length))\n",
        "\n",
        "# create sample tables with n-rows of [<int, string of 'l' chars>]\n",
        "def gen_random_rows(l, n, seed):\n",
        "  random.seed(seed)\n",
        "  return [(random.randint(1, 500), gen_random_string(l), '..') for i in range(n)]\n",
        "random.seed(3141)\n",
        "\n",
        "verbosity = Verbose.LISTPAGES\n",
        "\n",
        "\"\"\" Generate a Songs table with 100 tuples, with k=3 rows per page\"\"\"\n",
        "Songs = DbFile(values=gen_random_rows(2, 100, 3141), k=3, filepfx='Songs')\n",
        "\n",
        "algoV = Algos(4, verbose=True) # algorithms in verbose mode\n",
        "algo = Algos(4) # algorithms in non-verbose mode\n",
        "Songs.print_file()\n",
        "SongsSorted = algoV.BigSort(Songs) # Sort Songs table"
      ],
      "metadata": {
        "id": "TIO3VDO2kOb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Generate a Listens table with 100 tuples, with k=7 rows per page\"\"\"\n",
        "verbosity = Verbose.LISTPAGES\n",
        "\n",
        "Listens = DbFile(values=gen_random_rows(2, 100, 4242), k=7, filepfx='Listens')\n",
        "Listens.print_file()\n",
        "\n",
        "\n",
        "ListensSorted = algoV.BigSort(Listens) # Sort Listens table"
      ],
      "metadata": {
        "id": "d3OJSQIhB-Bb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "algoV.BNLJ(Songs, Listens, 4)"
      ],
      "metadata": {
        "id": "bbUYIDbZBsa2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "algoV.HPJ(Songs, Listens, 4)"
      ],
      "metadata": {
        "id": "qgq_PPzALKq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "algoV.SMJ(Songs, Listens, 4)"
      ],
      "metadata": {
        "id": "n7S08yHbLPMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Section 3] JOINs, GROUPBY IO Costs\n",
        "\n",
        "[Material from Lectures 5, 6, 7, 8]\n",
        "\n",
        "We need to model a real-world complex system. We use a simple model for good approximations of IO costs (and not get bogged down by other factors -- e.g., data caching, contentions in IO, etc.)"
      ],
      "metadata": {
        "id": "otx_Ri9Oi-Ud"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fud-uPcq27Iw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################################\n",
        "## IO Cost Equations\n",
        "################################################\n",
        "class DBOptimizer:\n",
        "  # setting up IO, e.g, Read/Write costs in IOs\n",
        "  def __init__(self, B, C_r, C_w):\n",
        "    self.B = B\n",
        "    (self.C_w, self.C_r) = (C_w, C_r)\n",
        "\n",
        "  ##########\n",
        "  # Sort(R) and HP(R)\n",
        "  def SortCost(self, R):\n",
        "    # assume repacking optimization, and B-way merge. Assume B ~= B+1\n",
        "    P_R = R.P()\n",
        "    if P_R == 0:\n",
        "      return 0\n",
        "    return (self.C_r+self.C_w)*P_R*(ceil(log(P_R/(2*self.B), self.B)) + 1)\n",
        "\n",
        "  def HPCost(self, R):\n",
        "    # C_r*R.P() to read pages, and C_w*R.P() to write out partition\n",
        "    return ((self.C_r+self.C_w)*R.P())\n",
        "\n",
        "  def Sort(self, R):\n",
        "    R.Sort()\n",
        "    return self.SortCost(R)\n",
        "  def HP(self, R):\n",
        "    R.HP()\n",
        "    return self.HPCost(R)\n",
        "\n",
        "  #########\n",
        "  #### JOIN Algorithms\n",
        "  #### We'll handle OUT outside the function.\n",
        "  # JOIN Algo1: P(R), P(S) and B: comute BNLJ IO Cost.\n",
        "  def BNLJCost(self, R, S):\n",
        "    ioCost = self.C_r*(R.P() + S.P()*ceil(R.P()/self.B))\n",
        "    if R == S: # self-join: assume no special optimization.\n",
        "      pass     # TODO(student): Think if you can improve BNLJ for self-joins?\n",
        "    return ioCost\n",
        "\n",
        "  # JOIN Algo2: P(R), P(S), B. Are R and S sorted?\n",
        "  # Should we assume small backups (linear? Depends on duplicates, and small B)?\n",
        "  def SMJCost(self, R, S, smallBackup):\n",
        "    ioCost = 0.0\n",
        "    if (not R.isSorted): # then eval sort R\n",
        "      ioCost += self.SortCost(R)\n",
        "    if (not S.isSorted): # then eval sort S\n",
        "      ioCost += self.SortCost(S)\n",
        "    if (smallBackup): # enuf B, and non-duplicates\n",
        "      ioCost += R.P() + S.P()\n",
        "    else:  # assume worst case, and doing BNLJ\n",
        "      ioCost += min(self.BNLJCost(R, S), self.BNLJCost(S, R))\n",
        "    if (R == S): # self-join? Halve the cost. No need to Sort(R) twice, etc.\n",
        "      ioCost /= 2\n",
        "    return ioCost\n",
        "\n",
        "  # JOIN Algo 3: P(R), P(S), B. Are R and S hash partioned?\n",
        "  # Should we assume few collisions (linear? Depends on hash function, and B)\n",
        "  def HPJCost(self, R, S, fewCollisions):\n",
        "    ioCost = 0.0\n",
        "    if (not R.isHPed): # then eval hash-partition R\n",
        "      ioCost += self.HPCost(R)\n",
        "    if (not S.isHPed): # then eval hash-partition S\n",
        "      ioCost += self.HPCost(S)\n",
        "    if fewCollisions:\n",
        "      ioCost += (R.P() + S.P())\n",
        "    else: # worst-case,  and doing BNLJ\n",
        "      ioCost += min(self.BNLJCost(R, S), self.BNLJCost(S, R))\n",
        "    if (R == S): # self-join? Halve the cost. Don't need to HP twice, etc.\n",
        "      ioCost /= 2\n",
        "    return ioCost\n",
        "\n",
        "  def JoinOUTEstimator(self, R, S, k_WHERE, special = ''):\n",
        "    # In general, this is a hard problem to model accurately. DBs keep\n",
        "    # a variety of stats (from prior queries) to predict the size of results.\n",
        "    # Let's sketch some rough ideas to get some flavor\n",
        "    #     1. What's the rowsize of OUT?\n",
        "    #        Simple model = add R.rowsize + S.rowsize\n",
        "    #        More accurate model: factor in specific columns projected.\n",
        "    #     2. What's the # rows in OUT?\n",
        "    #       (a) Simple model = For general tables,\n",
        "    #           Worst case: R.T()*S.T() rows.\n",
        "    #           With WHERE filtering: use probability of row satisfying WHERE\n",
        "    #       (b) Special cases: When JOIN key is unique (e.g. JOIN on studentID),\n",
        "    #           OUT will have appx min(R.T(), S.T()) rows.\n",
        "    #       (c) For other special cases (e.g., outerjoin, with WHERE, etc.)\n",
        "    #           model using probability\n",
        "    #     We'll use a simple estimator.\n",
        "    if (R == S):\n",
        "      sizeRowOUT = R.RowSize()\n",
        "    else:\n",
        "      sizeRowOUT = R.RowSize() + S.RowSize()\n",
        "    numRowsOUT = k_WHERE*R.T()*S.T()\n",
        "    if (special == 'uniq'): # fewer tuples\n",
        "      numRowsOUT = min(R.T(), S.T())*k_WHERE\n",
        "    return (numRowsOUT, sizeRowOUT)\n",
        "\n",
        "  def GroupBYEstimator(self, R, k_HAVING):\n",
        "    # Same ideas as JOIN estimator\n",
        "    sizeRowOUT = R.RowSize()\n",
        "    numRowsOUT = R.T()*k_HAVING\n",
        "    return (numRowsOUT, sizeRowOUT)\n",
        "\n",
        "  ## GROUPBY on Column X.\n",
        "  # GROUPBY Algo1: Sorting R (on columns X) will group rows in sorted order.\n",
        "  # We can then scan the groups to compute aggregates.\n",
        "  def GroupBySort(self, R):\n",
        "    return self.SortCost(R)\n",
        "\n",
        "  # GROUPBY Algo2: Hash partititioning R (on columns X) will place relevant groups\n",
        "  # in the same bucket, making it easy to compute aggregates.\n",
        "  def GroupByHash(self, R):\n",
        "    return self.HPCost(R)\n",
        "\n",
        "  ## Basic \"v1\" query optimizer\n",
        "  # DB checks cost of all plans. Picks lowest cost, and executes that plan.\n",
        "  # 1. In this \"v1\", we assume (a) no backup, and (b) no data skew for simplicity.\n",
        "  # In a more sophisticated version, the optimer will keep stats to estimate\n",
        "  # the shape of the data (e.g., duplicates, skew, etc.).\n",
        "  # 2. Also, we ignore OUT because all the JOINs will produce the same result,\n",
        "  # and will cost the same to output the OUT.\n",
        "\n",
        "  def EvaluateJoinPlans(self, R, S):\n",
        "    plans = {}\n",
        "    plans[\"BNLJ\"] = self.BNLJCost(R, S)\n",
        "    plans[\"BNLJ-rev\"] = self.BNLJCost(S, R)\n",
        "    plans[\"SMJ\"] = self.SMJCost(R, S, True)\n",
        "    plans[\"SMJ-BadBackup\"] = self.SMJCost(R, S, False)\n",
        "    plans[\"HPJ\"] = self.HPJCost(R, S, True)\n",
        "    plans[\"HPJ-BadSkew\"] = self.HPJCost(R, S, False)\n",
        "\n",
        "    return plans\n",
        "\n",
        "  def EvaluateGroupByPlans(self, R):\n",
        "    plans = {}\n",
        "    plans[\"GroupBy-Sort\"] = self.GroupBySort(R)\n",
        "    plans[\"GroupBy-Hash\"] = self.GroupByHash(R)\n",
        "    return plans\n",
        "\n",
        "  def QueryOptimizerV1(self, R, S):\n",
        "    plans = self.EvaluateJoinPlansV1(R, S)\n",
        "    print(plans)\n"
      ],
      "metadata": {
        "id": "UmnYS4mg92gT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, HTML\n",
        "import pandas as pd\n",
        "\n",
        "def addToPd(desc, plans):\n",
        "  return([desc, plans.get(\"BNLJ\"), plans.get(\"BNLJ-rev\"),\n",
        "          plans.get(\"SMJ\"), plans.get(\"SMJ-BadBackup\"),\n",
        "          plans.get(\"HPJ\"), plans.get(\"HPJ-BadSkew\"),\n",
        "          plans.get(\"GroupBy-Sort\"), plans.get(\"GroupBy-Hash\"),\n",
        "          plans.get(\"OUT\"), plans.get(\"OUT-uniq\")])\n",
        "\n",
        "ex = []\n",
        "db100 = DBOptimizer(100, 1.0, 1.0)\n",
        "# Example1: Evaluate JOINing two non-big tables\n",
        "Songs1 = Table(10.0*GB, 1024.0*Bytes)\n",
        "Listens1 = Table(10.0*GB, 1024.0*Bytes)\n",
        "ex.append(addToPd(\"Ex1 J(Songs1,Listens1)\",\n",
        "                  db100.EvaluateJoinPlans(Songs1, Listens1)))\n",
        "\n",
        "# Example2: Evaluate JOINing two big tables\n",
        "Songs2 = Table(100.0*GB, 1024.0*Bytes)\n",
        "Listens2 = Table(2.0*TB, 1024.0*Bytes)\n",
        "ex.append(addToPd(\"Ex2 J(Songs2,Listens2)\",\n",
        "                  db100.EvaluateJoinPlans(Songs2, Listens2)))\n",
        "\n",
        "# Example3: Assume Songs2 and Listens2 are sorted.\n",
        "Songs2.Sort()\n",
        "Listens2.Sort()\n",
        "ex.append(addToPd(\"Ex3 J(Songs2.sort,Listens2.sort)\",\n",
        "                  db100.EvaluateJoinPlans(Songs2, Listens2)))\n",
        "\n",
        "# Example4: Self-join\n",
        "Songs2.Reset()\n",
        "Listens2.Reset()\n",
        "ex.append(addToPd(\"Ex4 J(Listens2, Listens2)\",\n",
        "                  db100.EvaluateJoinPlans(Listens2, Listens2)))\n",
        "\n",
        "# Example5: from Spotify song_similarity CTE\n",
        "listens_rowSize = 32.0*Bytes\n",
        "listens_numRows = pow(10.0, 11) # 100 billion listens\n",
        "listens_sizeinMBs = listens_rowSize*listens_numRows/MB # ~= 300 GBs\n",
        "Listens = Table(listens_sizeinMBs, 32.0*Bytes)\n",
        "ex.append(addToPd(\"Ex5 SongSimilarity\",\n",
        "                  db100.EvaluateJoinPlans(Listens, Listens)))\n",
        "\n",
        "pdf = pd.DataFrame(ex, columns= [\"Example\", \"BNLJ\", \"BNLJ-rev\", \"SMJ\",\n",
        "                                 \"SMJ-BadBackup\", \"HPJ\", \"HPJ-BadSkew\",\n",
        "                                 \"GroupBy-Sort\", \"GroupBy-Hash\",\n",
        "                                 \"OUT\", \"OUT-uniq\"])\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "pdf.plot.bar(x='Example', y = ['BNLJ', 'BNLJ-rev', 'SMJ', 'SMJ-BadBackup',\n",
        "                               'HPJ', 'HPJ-BadSkew',\n",
        "                               'GroupBy-Sort', 'GroupBy-Hash', 'OUT', 'OUT-uniq'],\n",
        "                               figsize=(10,8), logy=True)\n",
        "plt.ylabel(\"IOCost\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_lrRQlKojIXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**OBSERVATIONS**\n",
        "\n",
        "1. In Example 1, BNLJ is cheaper than SMJ. Why? Songs1 and Listens1 (10 GBs) are pretty small vs B.(100 page = 100*64MBs = 6.4GBs). Intuitively, makes sense. That is, if the data mostly fits in RAM, BNLJ will JOIN in RAM, without any pre-processing (Sort or HP) overhead.\n",
        "\n",
        "2. In other cases, we see SMJ and HPJ are doing better than BNLJ. Especially, when the number of pages in join tables is big compared to B. Of course, in the worst case, SMJ (with bad backbackup) or HPJ (with bad skew) could perform poorly. SMJ and HPJ do even better, if they were pre-sorted or pre-partitioned (perhaps for another query or index).\n",
        "\n",
        "3. In practice, the query optimizer will evaluate the costs, and pick the algorithm with the least expected cost (e.g. based on prior history or statistics it maintains about the likelihood of duplicates.)"
      ],
      "metadata": {
        "id": "hw3Axz_DjMcI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2 Problems\n",
        "Consider Songs3=100GB and Listens3 = 1.0TB. Both have row size = 1024.0 Bytes. Let's explore the impact of different machine configurations on IO costs.\n",
        "1. Machines have different RAM buffer sizes (e.g., 32 GBs to 640GBs). Also, when handling multiple parallel queries, query optimizers may use only a portion of the available RAM per query (e.g. 20% for one query, 60% for another, etc).\n",
        "2. IOdevices often have different C_r and C_w costs. In some devices, C_w is 10x\n",
        "as much as C_r, and in some devices C_w is much faster than C_r.  "
      ],
      "metadata": {
        "id": "gNLXfxmt0Rt-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Problem 2.1:\n",
        "Compute IOcosts of BNLJ, SMJ, HPJ for B = 10, 100, 1000, 10000, for different\n",
        "values of C_r and C_w. Ignore OUT (cost of writing output) in these calculations.\n",
        "What are the relative costs of SMJ and HPJ versus BNLJ?\n",
        "\"\"\"\n",
        "Songs3 = Table(100.0*GB, 1024.0*Bytes)\n",
        "Listens3 = Table(1.0*TB, 1024.0*Bytes)\n",
        "\n",
        "print(f'{Songs3.P()=} {Listens3.P()=}')\n",
        "costs = []\n",
        "for B in [10, 100, 1000, 10000]:\n",
        "  for C_r in [1.0, 10.0]:\n",
        "    for C_w in [1.0, 10.0]:\n",
        "      db = DBOptimizer(B, C_r, C_w)\n",
        "      bnlj = db.BNLJCost(Songs3, Listens3)\n",
        "      smjbest = db.SMJCost(Songs3, Listens3, smallBackup=True)\n",
        "      smjworst = db.SMJCost(Songs3, Listens3, smallBackup=False)\n",
        "      hpjbest = db.HPJCost(Songs3, Listens3, fewCollisions=True)\n",
        "      hpjworst = db.HPJCost(Songs3, Listens3, fewCollisions=False)\n",
        "      costs.append([B, C_r, C_w, bnlj, smjbest/bnlj, smjworst/bnlj,\n",
        "                    hpjbest/bnlj, hpjworst/bnlj])\n",
        "pdf = pd.DataFrame(costs, columns=['B', 'C_r', 'C_w', 'BNLJ',\n",
        "                                   'SMJbest', 'SMJworst', 'HPJbest', 'HPJworst'])\n",
        "print(pdf)"
      ],
      "metadata": {
        "id": "JG_3QFKDNaiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kePwE5ZWLQir"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sJfZjZs2LKRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Section 2i] Indexing"
      ],
      "metadata": {
        "id": "q0vAsMX9arwf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Indexing and querying algorithms\n",
        "\"\"\"\n",
        "class IndexFile(DbFile):\n",
        "  def __init__(self, data, outk):\n",
        "    self.idxType = 'basic'\n",
        "    self.numIOs = 0\n",
        "    self.outindex = ''\n",
        "    self.algo = Algos(4)\n",
        "    self.R = data\n",
        "    self.outk = outk\n",
        "\n",
        "  \"\"\"Build a basic index, given a DbFile.\n",
        "    Each index entry contains (value, page#). The entry indicates\n",
        "    the value and the page# of the value in the given DbFile.\n",
        "        Step 1: Read each page in DbFile.\n",
        "        Step 2: Add (value, page#) to index, for each value in page.\n",
        "        Step 3: Sort the final index, for easy searching\n",
        "    Possible extensions --\n",
        "      1. Compress the index by storing blooom filters instead of value.\n",
        "      2. For simplicity of code, we're indexing the 0th column. We can create\n",
        "      indexes on combinations (e.g. on two or more columns)\"\"\"\n",
        "  def MakeBasicIndex(self, sort=True, level=1):\n",
        "    tmpindex = DbFile(values=[], k=self.outk,\n",
        "                                  filepfx=self.R.filepfx +f'.idx')\n",
        "    for i in range(self.R.P): # Step 1\n",
        "      page = self.R.read_page(i)\n",
        "      for j in range(len(page)):\n",
        "        if self.R.is_row_valid(page[j]): # Step 2\n",
        "        # For simplicity, index only the 0th column\n",
        "        # In general, index (value -> value's page#)\n",
        "          tmpindex.append_page((page[j][0], i))\n",
        "    if sort: # Step 3\n",
        "      self.idxType = 'basicSorted'\n",
        "      self.outindex = self.algo.BigSort(tmpindex)[0]\n",
        "    else:\n",
        "      self.outindex = tmpindex\n",
        "    return self.outindex\n",
        "\n",
        "  \"\"\"Create an index over a Sorted DbFile\n",
        "     Index only the 1st value in each sorted page.\n",
        "     E.g., val_i in page_i and val_j in page_j.\n",
        "     When searching for a value,\n",
        "     only values between val_i and val_j will be in page_i.\"\"\"\n",
        "  def MakeIndexForSortedData(self, level=1):\n",
        "    self.idxType = 'onSortedData'\n",
        "    tmpindex = DbFile(values=[], k=self.outk,\n",
        "                      filepfx=self.R.filepfx + f'.idx')\n",
        "    for i in range(self.R.P):\n",
        "      page = self.R.read_page(i)\n",
        "      if self.R.is_row_valid(page[0]):\n",
        "        tmpindex.append_page((page[0][0], i))\n",
        "    self.outindex = self.algo.BigSort(tmpindex)[0]\n",
        "    return self.outindex\n",
        "\n",
        "  \"\"\" Make a multi-level index.\n",
        "      Create R.idx for table R. Create R.idx.idx for R.idx, and so on.\n",
        "      Index the data as usual. Organize the index, for faster navigation.\"\"\"\n",
        "  def MakeBPlusTreeIndex(self, fanout, clustered=False):\n",
        "    self.idxType = 'b+tree'\n",
        "    level=1\n",
        "    b = IndexFile(self.R, fanout)\n",
        "    if clustered:\n",
        "      self.bindex = [b.MakeIndexForSortedData(level=level)]\n",
        "    else:\n",
        "      self.bindex = [b.MakeBasicIndex(level=level)]\n",
        "    while (self.bindex[-1].P > 1):\n",
        "      n = IndexFile(self.bindex[-1], fanout)\n",
        "      level += 1\n",
        "      next = n.MakeIndexForSortedData(level=level)\n",
        "      next.print_file()\n",
        "      self.bindex += [next]\n",
        "    self.bindex.reverse()\n",
        "\n",
        "  def print_file(self):\n",
        "    if self.idxType == 'b+tree':\n",
        "      for idx in self.bindex:\n",
        "        idx.print_file()\n",
        "    else:\n",
        "      self.outindex.print_file()\n",
        "\n",
        "  def GetData(self, kval, datapagenum):\n",
        "    datapage = self.R.read_page(datapagenum)\n",
        "    for i in range(len(datapage)):\n",
        "      if self.R.is_row_valid(datapage[i]):\n",
        "        row = datapage[i]\n",
        "        if row[0] == kval: # indexing on 1st column\n",
        "          print(f'Found {row=}')\n",
        "          return row\n",
        "    return None\n",
        "\n",
        "  \"\"\" Find kval in index\n",
        "      For querying for a needed value, we'll see code to\n",
        "        -Scan the index to find the page# with the value. Read that data page\n",
        "        -If the value is not found in the index, the value is not in the data.\n",
        "     Note: The code below works when the data does not have duplicates.\n",
        "     Modiyfing to handle duplicates is easy once you understand this version\"\"\"\n",
        "  def QueryIndex(self, kval):\n",
        "    idx = self.outindex\n",
        "    self.numIOs = 0\n",
        "    # Step 1: Walk through Index pages in sequence\n",
        "    # Step 2: If Index is sorted, stop when we find a bigger value\n",
        "    for page in idx.read_all_pages():\n",
        "      self.numIOs += 1\n",
        "      for idxval, pagenum in page: ## process values in RAM\n",
        "        if idxval != None and kval == idxval:\n",
        "          return self.numIOs, self.GetData(kval, pagenum)\n",
        "        if idxval > kval and self.idxType in ['basicSorted', 'onSortedData']:\n",
        "          # if sorted, can stop searching. Value not found\n",
        "          return self.numIOs, None\n",
        "    return self.numIOs, None\n",
        "\n",
        "  \"\"\" Find kval in B+ tree index\n",
        "      Note: The code below works when the index does not have duplicates\"\"\"\n",
        "  def QueryBplusIndex(self, kval):\n",
        "    idxpagenum = 0\n",
        "    self.numIOs = 0\n",
        "    # Step 1: Walk through the tree from root to leaf nodes.\n",
        "    # Step 2: Read the data page, using pagenum from the leaf node\n",
        "    for idx in self.bindex: # Step 1\n",
        "      page = idx.read_page(idxpagenum)\n",
        "      self.numIOs += 1\n",
        "      for idxval, pagenum in page:\n",
        "        if idxval != None and idxval <= kval:\n",
        "          idxpagenum = pagenum # Track the next level\n",
        "    # Step 2\n",
        "    return self.numIOs, self.GetData(kval, idxpagenum)\n",
        "\n",
        "def BplusTreeCost(num_search_keys, key_size, pointer_size,\n",
        "                  page_size):\n",
        "  fanout = math.floor(page_size/(key_size + pointer_size))\n",
        "  idx_height = math.ceil(math.log(num_search_keys, fanout))\n",
        "  return fanout, idx_height\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "15y6LwBszkwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Section 2i.1] Examples for Indexing"
      ],
      "metadata": {
        "id": "3RNlWLSijm14"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Example1: Make a basic index for Songs\"\"\"\n",
        "s1index = IndexFile(Songs, 14)\n",
        "Songs.print_file()\n",
        "s1index.MakeBasicIndex()\n",
        "s1index.print_file()"
      ],
      "metadata": {
        "id": "k_s_i7x422HV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Example2: Sort Songs data and then build an index\"\"\"\n",
        "SortedSongs = algo.BigSort(Songs)[0]\n",
        "SortedSongs.print_file()\n",
        "\n",
        "s2index = IndexFile(SortedSongs, 14)\n",
        "s2index.MakeIndexForSortedData()\n",
        "s2index.print_file()"
      ],
      "metadata": {
        "id": "5kogMXauj8dc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Example3: Build B+ index on Songs data\"\"\"\n",
        "s3index = IndexFile(Songs, 14)\n",
        "s3index.MakeBPlusTreeIndex(14)\n",
        "s3index.print_file()"
      ],
      "metadata": {
        "id": "Z_rrmZgKkCON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Example4: Build a B+ index on sorted Songs table\"\"\"\n",
        "s4index = IndexFile(SongsSorted[0], 14)\n",
        "s4index.MakeBPlusTreeIndex(14, clustered=True)\n",
        "s4index.print_file()"
      ],
      "metadata": {
        "id": "xFlPiuWgkmbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, HTML\n",
        "import pandas as pd\n",
        "from decimal import Decimal\n",
        "\n",
        "pointer_size = 8.0\n",
        "for page_size in [64.0*KB, 64.0*MB]:\n",
        "  print(f'Page Size (MBs): {page_size}')\n",
        "  ex = []\n",
        "  for key_size in [8.0, 1024]:\n",
        "    for num_search_keys in [pow(10.0, 6), pow(10.0, 12)]:\n",
        "      fanout, idx_height = BplusTreeCost(num_search_keys, key_size*Bytes,\n",
        "                                         pointer_size*Bytes, page_size)\n",
        "      idx_size = page_size*num_search_keys/fanout\n",
        "      ex.append([Decimal(str(num_search_keys/pow(10.0, 6))),\n",
        "                 key_size, Decimal(str(idx_size/GB)),\n",
        "                 fanout, idx_height])\n",
        "  pdf = pd.DataFrame(ex, columns = [\"Num search keys (in millions)\",\n",
        "                                    \"Search key size\", \"Index size (GBs)\", \"Fanout\", \"Height\"])\n",
        "  display(pdf)\n"
      ],
      "metadata": {
        "id": "AGO-Tpy25otv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "numIOs, val = s1index.QueryIndex(499)\n",
        "print(f'Num IOs: (index) {numIOs} + (data) 1')\n",
        "\n",
        "numIOs, val = s2index.QueryIndex(499)\n",
        "print(f'Num IOs: (index) {numIOs} + (data) 1')\n",
        "\n",
        "numIOs, val = s3index.QueryBplusIndex(499)\n",
        "print(f'Num IOs: (index) {numIOs} + (data) 1')\n",
        "\n"
      ],
      "metadata": {
        "id": "tJ08r6cqY55m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Section 4] Key-Values Stores"
      ],
      "metadata": {
        "id": "t_SQWirUQs8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Sorted String Table (SSTable): a key-value data struct in which keys are sorted\n",
        "\"\"\"\n",
        "class SSTable(DbFile):\n",
        "  def __init__(self, k, filepfx):\n",
        "    super().__init__(k=k, filepfx=filepfx)\n",
        "\n",
        "  def add(self, key, value):\n",
        "    self.append_page((key, value))\n",
        "\n",
        "  def get(self, key):\n",
        "    \"\"\"Idea: Return the value associated with a key.\n",
        "    If the SSTable is indexed, lookup in the IndexFile.\n",
        "    Otherwise, scan the DbFile\"\"\"\n",
        "    return None\n",
        "\n",
        "\"\"\"\n",
        "A Log-Structured Merge-tree (LSM Tree) is a disk-based data structure\n",
        "to provide low latency for a high rate of key-value inserts and deletes.\n",
        "\"\"\"\n",
        "class LSMTree:\n",
        "    def __init__(self, k):\n",
        "        self.memtable = {}\n",
        "        self.sstables = []\n",
        "        self.indexfiles = None\n",
        "        self.k = k\n",
        "        self.algos = Algos(B=4)\n",
        "\n",
        "    # Flush the MemTable to an SSTable when MemTable gets full\n",
        "    def flush(self):\n",
        "        sstable = SSTable(self.k, filepfx = f'sst.{len(self.sstables)}')\n",
        "        # sort keys before adding to SSTable\n",
        "        memsorted = sorted(self.memtable.items())\n",
        "        print(f'Create sstable...[sst.{len(self.sstables)}]: ' + \\\n",
        "              'from memtable')\n",
        "        for key, value in memsorted:\n",
        "            sstable.add(key, value)\n",
        "        sstable.print_file()\n",
        "        self.sstables.append(sstable)\n",
        "        self.memtable = {}\n",
        "        if (len(self.sstables) > 5):\n",
        "          self.merge()\n",
        "\n",
        "    # Merge multiple SSTables into one\n",
        "    def merge(self):\n",
        "        displaySectionCaption(\"Merging SSTables\", color='blue')\n",
        "        new_sst = self.algos.mergeBway(self.sstables)\n",
        "        self.sstables = new_sst\n",
        "        for sst in new_sst:\n",
        "            print(sst.print_file())\n",
        "        self.sstindex = IndexFile(self.sstables[0], 14)\n",
        "        self.sstindex.MakeIndexForSortedData()\n",
        "        self.sstindex.print_file()\n",
        "\n",
        "    def insert(self, key, value):\n",
        "        self.memtable[key] = value\n",
        "        if len(self.memtable) >= 10:  # Simplified condition for flushing\n",
        "            self.flush()\n",
        "\n",
        "    # Retrieve the value for a key\n",
        "    def get(self, key):\n",
        "        # Check memtable. If not, check sstables\n",
        "        value = self.memtable.get(key, None)\n",
        "        if value is not None:\n",
        "            return value\n",
        "        for sstable in reversed(self.sstables):\n",
        "            value = sstable.get(key)\n",
        "            if value is not None:\n",
        "                return value\n",
        "        return None"
      ],
      "metadata": {
        "id": "-L30T3N1Qqhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "values = gen_random_rows(2, 200, 3141)\n",
        "print(values)\n",
        "\n",
        "LSM = LSMTree(k=10)\n",
        "for v in values:\n",
        "  (id, key, rest) = v\n",
        "  LSM.insert(key, id)\n",
        "for sst in LSM.sstables:\n",
        "  #print(sst.data)\n",
        "  print(sst.print_file())\n"
      ],
      "metadata": {
        "id": "FpeD7dbCayDg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}